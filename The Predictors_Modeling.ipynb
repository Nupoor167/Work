{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba2a98f-3b6b-4040-ab72-f92a2eaa6198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading the necessary libraries: #base model\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import hstack \n",
    "import csv\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,classification_report\n",
    "\n",
    "#seeding for randamziation:\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbd166c-9ba6-4c33-8770-9f12ae005771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading raw data from Final_Annotationsby_Detectives_GSD.csv Golden Standards dataset: \n",
    "#Feature and Label Variables:\n",
    "X_txt= []\n",
    "y= []\n",
    "\n",
    "# Loading data from CSVs:\n",
    "# Load the training datasets into two lists (X_txt will be a list of strings with features; \n",
    "# y will list of 0's and 1's with labels or classification):\n",
    "with open('./Gold Standards DataSet_D_SA.csv',encoding='iso-8859-1') as in_file:\n",
    "    iCSV = csv.reader(in_file, delimiter=',')\n",
    "    header=next(iCSV)\n",
    "    for row in iCSV:\n",
    "        X_txt.append(row[1])\n",
    "        y.append([int(value) for value in row[2:6]])\n",
    "        \n",
    "#print(len(X_txt))\n",
    "#print(len(y))\n",
    "#print(header[1:6],y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617a88a1-a2d5-4d0b-8cb9-a9615d93877c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Split the data into training and test sets:\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_txt,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb541765-83d9-4625-8346-6ef04a6014e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Validation Score F1: 0.3262\n",
      "Macro Test Score F1: 0.3120\n",
      "Macro_Score Precision: 0.3233\n",
      "Macro_Score Recall: 0.3078\n",
      "Gold Standards- Technology Macro_Score F1: 0.8201\n",
      "Gold Standards- Ride Share Macro_Score F1: 0.0909\n",
      "Gold Standards- Food Delivery Macro_Score F1: 0.1647\n",
      "Gold Standards- Online Shopping Macro_Score F1: 0.1724\n",
      "\n",
      "Micro Validation Score F1: 0.5797\n",
      "Micro Test Score F1: 0.5450\n",
      "Micro_Score Precision: 0.6124\n",
      "Micro_Score Recall: 0.4910\n",
      "Gold Standards- Technology Micro_Score F1: 0.7397\n",
      "Gold Standards- Ride Share Micro_Score F1: 0.0667\n",
      "Gold Standards- Food Delivery Micro_Score F1: 0.0000\n",
      "Gold Standards- Online Shopping Micro_Score F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#modeling with ngram_range=(1,1) & LogisticRegression with CountVectorizer: \n",
    "\n",
    "#converting list to matrix:\n",
    "vec=CountVectorizer(ngram_range=(1,1))\n",
    "X_train_matrix =vec.fit_transform(X_train) # This should be a matrix\n",
    "X_test_matrix=vec.transform(X_test)# This should be a matrix\n",
    "\n",
    "#vec=CountVectorizer(ngram_range=(1,1))\n",
    "#X_train_matrix =vec.fit_transform(X_txt) # This should be a matrix\n",
    "#y_matrix=vec.transform(y)# This should be a matrix\n",
    "\n",
    "#Split the data into training and test sets:\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train_matrix,y,test_size=0.2)\n",
    "\n",
    "#converting list to array:\n",
    "ya_train=np.array(y_train)\n",
    "ya_test=np.array(y_test)\n",
    "#print(y_test.shape[1])\n",
    "\n",
    "#initializing logisticregression:\n",
    "log_reg=MultiOutputClassifier(LogisticRegression(random_state=42,solver='lbfgs', max_iter=2000))\n",
    "\n",
    "#params with c values:\n",
    "params= {\"estimator__C\": [0.0001, 0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "#initialize GridSearchCV with scoring f1_macro:\n",
    "\n",
    "init_grid_search_macro=GridSearchCV(log_reg,params,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_macro:\n",
    "init_grid_search_macro.fit(X_train_matrix,ya_train)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_macro_score = init_grid_search_macro.score(X_test_matrix,ya_test) \n",
    "validation_results_best_score=init_grid_search_macro.best_score_\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Macro Validation Score F1: {:.4f}\".format(validation_results_best_score))\n",
    "print(\"Macro Test Score F1: {:.4f}\".format(validation_macro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_macro:\n",
    "logistic_X_test_prediciton_macro=init_grid_search_macro.predict(X_test_matrix)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as macro parameter:\n",
    "precision_macro=precision_score(ya_test,logistic_X_test_prediciton_macro,average='macro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_macro = recall_score(ya_test,logistic_X_test_prediciton_macro,average='macro')\n",
    "print(\"Macro_Score Precision: {:.4f}\".format(precision_macro))\n",
    "print(\"Macro_Score Recall: {:.4f}\".format(recall_macro))\n",
    "for i in range(4):\n",
    "    #f1_macro_i = f1_score([item[i] for item in y_test], [item[i] for item in logistic_X_test_prediciton_macro])\n",
    "    f1_macro_i = f1_score(ya_test[:,i],logistic_X_test_prediciton_macro[:,i])\n",
    "    print(f\"{header[i+2]} Macro_Score F1: {f1_macro_i:.4f}\")\n",
    "    \n",
    "print()    \n",
    "    \n",
    "\n",
    "#initialize GridSearchCV with scoring f1_micro:\n",
    "\n",
    "init_grid_search_micro=GridSearchCV(log_reg,params,cv=5,scoring='f1_micro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_micro:\n",
    "init_grid_search_micro.fit(X_train_matrix,ya_train)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_micro_score = init_grid_search_micro.score(X_test_matrix,ya_test) \n",
    "validation_results_best_score_micro=init_grid_search_micro.best_score_\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Micro Validation Score F1: {:.4f}\".format(validation_results_best_score_micro))\n",
    "print(\"Micro Test Score F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Micro f1:\n",
    "#print(\"Micro_Score Validation F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_micro:\n",
    "logistic_X_test_prediciton_micro=init_grid_search_micro.predict(X_test_matrix)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as micro parameter:\n",
    "precision_micro=precision_score(ya_test,logistic_X_test_prediciton_micro,average='micro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_micro = recall_score(ya_test,logistic_X_test_prediciton_micro,average='micro')\n",
    "print(\"Micro_Score Precision: {:.4f}\".format(precision_micro))\n",
    "print(\"Micro_Score Recall: {:.4f}\".format(recall_micro))\n",
    "for i in range(4):\n",
    "    #f1_micro_i = f1_score([item[i] for item in y_test], [item[i] for item in logistic_X_test_prediciton_micro])\n",
    "    f1_micro_i = f1_score(ya_test[:,i],logistic_X_test_prediciton_micro[:,i])\n",
    "    print(f\"{header[i+2]} Micro_Score F1: {f1_micro_i:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e7b3f2-fa4c-4be7-983c-dea27967491b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LexiconClassifier():\n",
    "    def __init__(self):\n",
    "        self.positive_words = set()\n",
    "        self.capital_words = set()\n",
    "        self.exclamation_points = set()\n",
    "        self.negative_words = set()\n",
    "        self.online_apps_words=set()\n",
    "        \n",
    "    def online_apps(self, file_path):\n",
    "        with open('./online-words.txt',encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.online_apps_words.add(row.strip())\n",
    "    def load_capital_words(self, file_path):\n",
    "        with open('./Gold Standards DataSet_D_SA.csv',encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.capital_words.add(row.strip())\n",
    "\n",
    "    def load_exclamation_points(self, file_path):\n",
    "        with open('./Gold Standards DataSet_D_SA.csv',encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.exclamation_points.add(row.strip())\n",
    "    def load_positive_negative_words(self, file_path):\n",
    "        with open('./Gold Standards DataSet_D_SA.csv',encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.positive_words.add(row.strip())\n",
    "                \n",
    "        with open('./Gold Standards DataSet_D_SA.csv', encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.negative_words.add(row.strip())\n",
    "                \n",
    "    def predict_positive_negtaive(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns a sentiment prediction give an input string.\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- a string (\"postive, \"negative\", or \"neutral\")\n",
    "        \"\"\"\n",
    "        num_pos_words = 0\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "            elif word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        \n",
    "        pred_pos_neg = 'neutral'        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            pred_pos_neg = 'positive'\n",
    "        elif num_pos_words < num_neg_words:\n",
    "            pred_pos_neg = 'negative'\n",
    "            \n",
    "        return pred_pos_neg\n",
    "    \n",
    "    \n",
    "    def predict_capital_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of capital words in a string.\n",
    "\n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is GOOD good GOOD\")\n",
    "\n",
    "            Returns:\n",
    "            num_capital_words -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_capital_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word.isupper() and word in self.capital_words:\n",
    "                num_capital_words += 1\n",
    "        return num_capital_words\n",
    "\n",
    "    def predict_exclamation_points(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of exclamation points in a string.\n",
    "\n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is great!!!\")\n",
    "\n",
    "            Returns:\n",
    "            num_exclamation_points -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_exclamation_points = 0\n",
    "        for char in sentence:\n",
    "            if char == '!':\n",
    "                num_exclamation_points += 1\n",
    "        return num_exclamation_points\n",
    "    \n",
    "    def count_pos_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of positive words in string\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_pos_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "        return num_pos_words\n",
    "    def count_neg_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of negative words in string\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        return num_neg_words\n",
    "    \n",
    "    def predict_online_apps(self, sentence):\n",
    "            \n",
    "        \"\"\"\n",
    "        Returns True if specific online application words are found in a string.\n",
    "\n",
    "        Keyword arguments:\n",
    "        sentence -- string (e.g., \"I ordered from ubereats and doordash yesterday.\")\n",
    "\n",
    "        Returns:\n",
    "        online_apps_present -- a boolean\n",
    "        \"\"\"\n",
    "        online_apps_count = sum(word in sentence.lower() for word in self.online_apps_words)\n",
    "        return online_apps_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802c9f5e-a46d-4afd-a8ee-ce5d39e06039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Instatiate that class\n",
    "\n",
    "lex_test_preds_P_N = [] # Initialize this as an empty list\n",
    "lex_test_preds_Capital = [] \n",
    "lex_test_preds_exclamation=[]\n",
    "lex_test_preds_positive_wrd=[]\n",
    "lex_test_preds_negative_wrd=[]\n",
    "lex_test_preds_online_apps_cnt=[]\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .predict() method\n",
    "#    append the prediction to lex_test_preds\n",
    "\n",
    "LexiconClassifier_v=LexiconClassifier()\n",
    "for i in X_test:\n",
    "    \n",
    "    #Predict positive and negative words in the sentence:\n",
    "    Predict_Positive_lexicon=LexiconClassifier_v.predict_positive_negtaive(i)\n",
    "    lex_test_preds_P_N.append(Predict_Positive_lexicon)\n",
    "    #Predict capital words in the senetence:\n",
    "    Predict_Capital_lexicon=LexiconClassifier_v.predict_capital_words(i)\n",
    "    lex_test_preds_Capital.append(Predict_Capital_lexicon)\n",
    "    #Predict exclamation(!) marks in the senetence:\n",
    "    Predict_exclamation_lexicon=LexiconClassifier_v.predict_exclamation_points(i)\n",
    "    lex_test_preds_exclamation.append(Predict_exclamation_lexicon)\n",
    "    #Predict count of positive words in the senetence:\n",
    "    Predict_positive_lexicon=LexiconClassifier_v.count_pos_words(i)\n",
    "    lex_test_preds_positive_wrd.append(Predict_positive_lexicon)\n",
    "    #Predict count of negative words in the senetence:\n",
    "    Predict_negative_lexicon=LexiconClassifier_v.count_neg_words(i)\n",
    "    lex_test_preds_negative_wrd.append(Predict_negative_lexicon)\n",
    "    #Predict count of negative words in the senetence:\n",
    "    Predict_online_apps_lexicon=LexiconClassifier_v.predict_online_apps(i)\n",
    "    lex_test_preds_online_apps_cnt.append(Predict_online_apps_lexicon)\n",
    "#print(lex_test_preds_exclamation[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdac016-3999-49e7-b6ee-fd2022b5f894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WRITE CODE HERE\n",
    "\n",
    "X_train_lexicon_features_pn = [] # Initailze to an empty list. This will be a list of li #  Initailze to an empty list. This will be a list of lists\n",
    "X_test_lexicon_features_pn=[]\n",
    "X_test_lexicon_features_em=[]\n",
    "X_train_lexicon_features_em=[]\n",
    "# Loop over X_txt_test\n",
    "#    for each string in X_txt_test (i.e., for each item in the list), pass it to LexiconClassifiers .count_pos_words() and count_neg_words method\n",
    "#    append a list with the counts to X_test_lexicon_features\n",
    "LexiconClassifier_v=LexiconClassifier()\n",
    "#count of positive and negative words:\n",
    "for i in X_test:\n",
    "    num_pos_test_count=LexiconClassifier_v .count_pos_words(i)\n",
    "    num_neg_test_count=LexiconClassifier_v .count_neg_words(i)\n",
    "    X_test_lexicon_features_pn.append([num_pos_test_count,num_neg_test_count])\n",
    "#print(X_test_lexicon_features)\n",
    "for j in X_train:\n",
    "    num_pos_train_count=LexiconClassifier_v .count_pos_words(j)\n",
    "    num_neg_train_count=LexiconClassifier_v .count_neg_words(j)\n",
    "    X_train_lexicon_features_pn.append([num_pos_train_count,num_neg_train_count])\n",
    "    \n",
    "#count of exclamation points:    \n",
    "for a in X_test:\n",
    "    num_exclamation_test_count=LexiconClassifier_v .predict_exclamation_points(a)\n",
    "    X_test_lexicon_features_em.append(num_exclamation_test_count)\n",
    "#print(X_test_lexicon_features_em)\n",
    "\n",
    "for b in X_train:\n",
    "    num_exclamation_train_count=LexiconClassifier_v .predict_exclamation_points(b)\n",
    "    X_train_lexicon_features_em.append(num_exclamation_train_count)\n",
    "#print(X_train_lexicon_features_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c03cfc-6f58-4c8a-b857-a8eb01e0ef01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Validation Score F1- Exclamation Feature: 0.3232\n",
      "Macro Test Score F1- Exclamation Feature: 0.3085\n",
      "Macro_Score Precision-Exclamation Feature: 0.3153\n",
      "Macro_Score Recall-Exclamation Feature: 0.3078\n",
      "Gold Standards- Technology Macro_Score F1-Exclamation Feature: 0.8099\n",
      "Gold Standards- Ride Share Macro_Score F1-Exclamation Feature: 0.0889\n",
      "Gold Standards- Food Delivery Macro_Score F1-Exclamation Feature: 0.1628\n",
      "Gold Standards- Online Shopping Macro_Score F1-Exclamation Feature: 0.1724\n",
      "\n",
      "Micro Validation Score F1-Exclamation Feature: 0.5803\n",
      "Micro Test Score F1-Exclamation Feature: 0.5464\n",
      "Micro_Score Precision-Exclamation Feature: 0.6158\n",
      "Micro_Score Recall-Exclamation Feature: 0.4910\n",
      "Gold Standards- Technology Micro_Score F1-Exclamation Feature: 0.7423\n",
      "Gold Standards- Ride Share Micro_Score F1-Exclamation Feature: 0.0667\n",
      "Gold Standards- Food Delivery Micro_Score F1-Exclamation Feature: 0.0000\n",
      "Gold Standards- Online Shopping Micro_Score F1-Exclamation Feature: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#modeling with count of exclamation marks ngram_range=(1,1) & LogisticRegression with CountVectorizer: \n",
    "\n",
    "#converting list to matrix for exclamation marks:\n",
    "vec_em=CountVectorizer(ngram_range=(1,1))\n",
    "X_train_matrix_em =vec_em.fit_transform(X_train) # This should be a matrix\n",
    "X_test_matrix_em=vec_em.transform(X_test)# This should be a matrix\n",
    "\n",
    "# Now we need to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "# \"hstack\" X_train_lexicon_features with X_train_w_lex\n",
    "# \"hstack\" X_test_lexicon_features with X_test_w_lex\n",
    "X_train_lexicon_farray_em=np.array(X_train_lexicon_features_em).reshape(-1, 1)\n",
    "X_test_lexicon_farray_em=np.array(X_test_lexicon_features_em).reshape(-1, 1)\n",
    "\n",
    "X_train_f_em_lex=hstack([X_train_matrix_em,X_train_lexicon_farray_em])\n",
    "X_test_f_em_lex=hstack([X_test_matrix_em,X_test_lexicon_farray_em])\n",
    "\n",
    "#converting list to array:\n",
    "ya_train_em=np.array(y_train)\n",
    "ya_test_em=np.array(y_test)\n",
    "#print(y_test.shape[1])\n",
    "\n",
    "#initializing logisticregression:\n",
    "log_reg_f_em=MultiOutputClassifier(LogisticRegression(random_state=42,solver='lbfgs', max_iter=2000))\n",
    "\n",
    "#params with c values:\n",
    "params_em= {\"estimator__C\": [0.0001, 0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "#initialize GridSearchCV with scoring f1_macro:\n",
    "\n",
    "init_grid_search_macro_f_em=GridSearchCV(log_reg_f_em,params_em,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_macro:\n",
    "init_grid_search_macro_f_em.fit(X_train_f_em_lex,ya_train_em)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_macro_score_f_em = init_grid_search_macro_f_em.score(X_test_f_em_lex,ya_test_em) \n",
    "validation_results_best_score_f_em=init_grid_search_macro_f_em.best_score_\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Macro Validation Score F1- Exclamation Feature: {:.4f}\".format(validation_results_best_score_f_em))\n",
    "print(\"Macro Test Score F1- Exclamation Feature: {:.4f}\".format(validation_macro_score_f_em))\n",
    "\n",
    "#predciting on X_test data with scoring f1_macro:\n",
    "logistic_X_test_prediciton_macro_f_em=init_grid_search_macro_f_em.predict(X_test_f_em_lex)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as macro parameter:\n",
    "precision_macro_f_em=precision_score(ya_test_em,logistic_X_test_prediciton_macro_f_em,average='macro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_macro_f_em = recall_score(ya_test_em,logistic_X_test_prediciton_macro_f_em,average='macro')\n",
    "print(\"Macro_Score Precision-Exclamation Feature: {:.4f}\".format(precision_macro_f_em))\n",
    "print(\"Macro_Score Recall-Exclamation Feature: {:.4f}\".format(recall_macro_f_em))\n",
    "for i_ma in range(4):\n",
    "    #f1_macro_i = f1_score([item[i] for item in y_test], [item[i] for item in logistic_X_test_prediciton_macro])\n",
    "    f1_macro_f_em_i_ma = f1_score(ya_test_em[:,i_ma],logistic_X_test_prediciton_macro_f_em[:,i_ma])\n",
    "    print(f\"{header[i_ma+2]} Macro_Score F1-Exclamation Feature: {f1_macro_f_em_i_ma:.4f}\")\n",
    "    \n",
    "print()    \n",
    "    \n",
    "\n",
    "#initialize GridSearchCV with scoring f1_micro:\n",
    "\n",
    "init_grid_search_micro_f_em=GridSearchCV(log_reg_f_em,params_em,cv=5,scoring='f1_micro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_micro:\n",
    "init_grid_search_micro_f_em.fit(X_train_f_em_lex,ya_train_em)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_micro_score_f_em = init_grid_search_micro_f_em.score(X_test_f_em_lex,ya_test_em) \n",
    "validation_results_best_score_micro_f_em=init_grid_search_micro_f_em.best_score_\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Micro Validation Score F1-Exclamation Feature: {:.4f}\".format(validation_results_best_score_micro_f_em))\n",
    "print(\"Micro Test Score F1-Exclamation Feature: {:.4f}\".format(validation_micro_score_f_em))\n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Micro f1:\n",
    "#print(\"Micro_Score Validation F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_micro:\n",
    "logistic_X_test_prediciton_micro_f_em=init_grid_search_micro_f_em.predict(X_test_f_em_lex)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as micro parameter:\n",
    "precision_micro_f_em=precision_score(ya_test_em,logistic_X_test_prediciton_micro_f_em,average='micro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_micro_f_em = recall_score(ya_test_em,logistic_X_test_prediciton_micro_f_em,average='micro')\n",
    "print(\"Micro_Score Precision-Exclamation Feature: {:.4f}\".format(precision_micro_f_em))\n",
    "print(\"Micro_Score Recall-Exclamation Feature: {:.4f}\".format(recall_micro_f_em))\n",
    "for i_mi in range(4):\n",
    "    #f1_micro_i = f1_score([item[i] for item in y_test], [item[i] for item in logistic_X_test_prediciton_micro])\n",
    "    f1_micro_f_em_i_mi = f1_score(ya_test_em[:,i_mi],logistic_X_test_prediciton_micro_f_em[:,i_mi])\n",
    "    print(f\"{header[i_mi+2]} Micro_Score F1-Exclamation Feature: {f1_micro_f_em_i_mi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c087cc-0839-4f1f-a9a0-3e5e8e8aa6f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_lexicon_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     30\u001b[0m X_train,X_test,y_train,y_test\u001b[38;5;241m=\u001b[39mtrain_test_split(X_train_matrix,y,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Now we need to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# \"hstack\" X_train_lexicon_features with X_train_w_lex\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# \"hstack\" X_test_lexicon_features with X_test_w_lex\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m X_train_lexicon_farray\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(X_train_lexicon_features)\n\u001b[0;32m     36\u001b[0m X_test_lexicon_farray\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(X_test_lexicon_features)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#converting list to array:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_lexicon_features' is not defined"
     ]
    }
   ],
   "source": [
    "#modeling with ngram_range=(1,1) & LogisticRegression with CountVectorizer: \n",
    "#reading the necessary libraries:\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,classification_report\n",
    "from scipy.sparse import hstack \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Generating Random multiplable classification for Logistic Regression:\n",
    "#X, y = make_multilabel_classification(n_classes=4,random_state=0)\n",
    "#lg_clf=LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "#clf = MultiOutputClassifier(lg_clf).fit(X, y)\n",
    "\n",
    "vec=CountVectorizer(ngram_range=(1,1))\n",
    "X_train_matrix =vec.fit_transform(X_txt) # This should be a matrix\n",
    "#y_matrix=vec.transform(y)# This should be a matrix\n",
    "\n",
    "#Split the data into training and test sets:\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_train_matrix,y,test_size=0.2)\n",
    "\n",
    "# Now we need to convert X_train_lexicon_features and X_test_lexicon_features to numpy arrays\n",
    "# \"hstack\" X_train_lexicon_features with X_train_w_lex\n",
    "# \"hstack\" X_test_lexicon_features with X_test_w_lex\n",
    "X_train_lexicon_farray=np.array(X_train_lexicon_features)\n",
    "X_test_lexicon_farray=np.array(X_test_lexicon_features)\n",
    "\n",
    "#converting list to array:\n",
    "ya_train=np.array(y_train)\n",
    "ya_test=np.array(y_test)\n",
    "#print(y_test.shape[1])\n",
    "#initializing logisticregression:\n",
    "log_reg=MultiOutputClassifier(LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000))\n",
    "\n",
    "#params with c values:\n",
    "params= {\"estimator__C\": [0.0001, 0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "#initialize GridSearchCV with scoring f1_macro:\n",
    "\n",
    "init_grid_search_macro=GridSearchCV(log_reg,params,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_macro:\n",
    "init_grid_search_macro.fit(X_train,ya_train)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_macro_score = init_grid_search_macro.score(X_test,ya_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Macro_Score Validation F1: {:.4f}\".format(validation_macro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_macro:\n",
    "logistic_X_test_prediciton_macro=init_grid_search_macro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as macro parameter:\n",
    "precision_macro=precision_score(ya_test,logistic_X_test_prediciton_macro,average='macro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_macro = recall_score(ya_test,logistic_X_test_prediciton_macro,average='macro')\n",
    "print(\"Macro_Score Precision: {:.4f}\".format(precision_macro))\n",
    "print(\"Macro_Score Recall: {:.4f}\".format(recall_macro))\n",
    "for i in range(4):\n",
    "    f1_macro_i = f1_score(ya_test[:,i],logistic_X_test_prediciton_macro[:,i],average='macro')\n",
    "    print(f\"{header[i+2]} Macro_Score F1: {f1_macro_i:.4f}\")\n",
    "    \n",
    "print()    \n",
    "    \n",
    "\n",
    "#initialize GridSearchCV with scoring f1_micro:\n",
    "\n",
    "init_grid_search_micro=GridSearchCV(log_reg,params,cv=5,scoring='f1_micro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_micro:\n",
    "init_grid_search_micro.fit(X_train,ya_train)\n",
    "\n",
    "#Validation Score with scoring f1_micro:\n",
    "validation_micro_score = init_grid_search_micro.score(X_test, ya_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Micro f1:\n",
    "print(\"Micro_Score Validation F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_micro:\n",
    "logistic_X_test_prediciton_micro=init_grid_search_micro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as micro parameter:\n",
    "precision_micro=precision_score(ya_test,logistic_X_test_prediciton_micro,average='micro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_micro = recall_score(ya_test,logistic_X_test_prediciton_micro,average='micro')\n",
    "print(\"Micro_Score Precision: {:.4f}\".format(precision_micro))\n",
    "print(\"Micro_Score Recall: {:.4f}\".format(recall_micro))\n",
    "for i in range(4):\n",
    "    f1_micro_i = f1_score(ya_test[:,i],logistic_X_test_prediciton_micro[:,i],average='micro')\n",
    "    print(f\"{header[i+2]} Micro_Score F1: {f1_micro_i:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "572ed487-0473-4f5c-8c2a-b84fac961019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro_Score Validation F1: 0.3034\n",
      "Macro_Score Precision: 0.3334\n",
      "Macro_Score Recall: 0.2923\n",
      "Gold Standards- Technology Macro_Score F1: 0.7765\n",
      "Gold Standards- Ride Share Macro_Score F1: 0.5211\n",
      "Gold Standards- Food Delivery Macro_Score F1: 0.5346\n",
      "Gold Standards- Online Shopping Macro_Score F1: 0.5179\n",
      "Micro_Score Validation F1: 0.6494\n",
      "Micro_Score Precision: 0.7692\n",
      "Micro_Score Recall: 0.5618\n",
      "Gold Standards- Technology Micro_Score F1: 0.7900\n",
      "Gold Standards- Ride Share Micro_Score F1: 0.8750\n",
      "Gold Standards- Food Delivery Micro_Score F1: 0.9350\n",
      "Gold Standards- Online Shopping Micro_Score F1: 0.8600\n"
     ]
    }
   ],
   "source": [
    "#modeling with ngram_range=(1,3) & LogisticRegression with CountVectorizer: \n",
    "#reading the necessary libraries:\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#np.random.seed(42)\n",
    "#import random\n",
    "#random.seed(42)\n",
    "\n",
    "# Generating Random multiplable classification for Logistic Regression:\n",
    "#X, y = make_multilabel_classification(n_classes=4,random_state=0)\n",
    "#lg_clf=LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "#clf = MultiOutputClassifier(lg_clf).fit(X, y)\n",
    "\n",
    "vec=CountVectorizer(ngram_range=(1,3))\n",
    "X_train_matrix =vec.fit_transform(X_txt) # This should be a matrix\n",
    "#y_matrix=vec.transform(y)# This should be a matrix\n",
    "\n",
    "#Split the data into training and test sets:\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train_matrix,y,test_size=0.2)\n",
    "\n",
    "#converting list to array:\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "#initializing logisticregression:\n",
    "log_reg=MultiOutputClassifier(LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000))\n",
    "\n",
    "#params with c values:\n",
    "params= {\"estimator__C\": [0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "#initialize GridSearchCV with scoring f1_macro:\n",
    "\n",
    "init_grid_search_macro=GridSearchCV(log_reg,params,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_macro:\n",
    "init_grid_search_macro.fit(X_train,y_train)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_macro_score = init_grid_search_macro.score(X_test, y_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Macro_Score Validation F1: {:.4f}\".format(validation_macro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_macro:\n",
    "logistic_X_test_prediciton_macro=init_grid_search_macro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as macro parameter:\n",
    "precision_macro=precision_score(y_test,logistic_X_test_prediciton_macro,average='macro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_macro = recall_score(y_test,logistic_X_test_prediciton_macro,average='macro')\n",
    "print(\"Macro_Score Precision: {:.4f}\".format(precision_macro))\n",
    "print(\"Macro_Score Recall: {:.4f}\".format(recall_macro))\n",
    "for i in range(4):\n",
    "    f1_macro_i = f1_score(y_test[:,i],logistic_X_test_prediciton_macro[:,i],average='macro')\n",
    "    print(f\"{header[i+2]} Macro_Score F1: {f1_macro_i:.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#initialize GridSearchCV with scoring f1_micro:\n",
    "\n",
    "init_grid_search_micro=GridSearchCV(log_reg,params,cv=5,scoring='f1_micro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_micro:\n",
    "init_grid_search_micro.fit(X_train,y_train)\n",
    "\n",
    "#Validation Score with scoring f1_micro:\n",
    "validation_micro_score = init_grid_search_micro.score(X_test, y_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Micro f1:\n",
    "print(\"Micro_Score Validation F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_micro:\n",
    "logistic_X_test_prediciton_micro=init_grid_search_micro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as micro parameter:\n",
    "precision_micro=precision_score(y_test,logistic_X_test_prediciton_micro,average='micro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_micro = recall_score(y_test,logistic_X_test_prediciton_micro,average='micro')\n",
    "print(\"Micro_Score Precision: {:.4f}\".format(precision_micro))\n",
    "print(\"Micro_Score Recall: {:.4f}\".format(recall_micro))\n",
    "for i in range(4):\n",
    "    f1_micro_i = f1_score(y_test[:,i],logistic_X_test_prediciton_micro[:,i],average='micro')\n",
    "    print(f\"{header[i+2]} Micro_Score F1: {f1_micro_i:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573954a9-7d89-4438-9907-1f498370320e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro_Score Validation F1: 0.3034\n",
      "Macro_Score Precision: 0.3334\n",
      "Macro_Score Recall: 0.2923\n",
      "Gold Standards- Technology Macro_Score F1: 0.7765\n",
      "Gold Standards- Ride Share Macro_Score F1: 0.5211\n",
      "Gold Standards- Food Delivery Macro_Score F1: 0.5346\n",
      "Gold Standards- Online Shopping Macro_Score F1: 0.5179\n",
      "Micro_Score Validation F1: 0.6494\n",
      "Micro_Score Precision: 0.7692\n",
      "Micro_Score Recall: 0.5618\n",
      "Gold Standards- Technology Micro_Score F1: 0.7900\n",
      "Gold Standards- Ride Share Micro_Score F1: 0.8750\n",
      "Gold Standards- Food Delivery Micro_Score F1: 0.9350\n",
      "Gold Standards- Online Shopping Micro_Score F1: 0.8600\n"
     ]
    }
   ],
   "source": [
    "#modeling with ngram_range=(1,5) & LogisticRegression with CountVectorizer: \n",
    "#reading the necessary libraries:\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#np.random.seed(42)\n",
    "#import random\n",
    "#random.seed(42)\n",
    "\n",
    "# Generating Random multiplable classification for Logistic Regression:\n",
    "#X, y = make_multilabel_classification(n_classes=4,random_state=0)\n",
    "#lg_clf=LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "#clf = MultiOutputClassifier(lg_clf).fit(X, y)\n",
    "\n",
    "vec=CountVectorizer(ngram_range=(1,5))\n",
    "X_train_matrix =vec.fit_transform(X_txt) # This should be a matrix\n",
    "#y_matrix=vec.transform(y)# This should be a matrix\n",
    "\n",
    "#Split the data into training and test sets:\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train_matrix,y,test_size=0.2)\n",
    "\n",
    "#converting list to array:\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "#initializing logisticregression:\n",
    "log_reg=MultiOutputClassifier(LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000))\n",
    "\n",
    "#params with c values:\n",
    "params= {\"estimator__C\": [0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "#initialize GridSearchCV with scoring f1_macro:\n",
    "\n",
    "init_grid_search_macro=GridSearchCV(log_reg,params,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_macro:\n",
    "init_grid_search_macro.fit(X_train,y_train)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_macro_score = init_grid_search_macro.score(X_test, y_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Macro_Score Validation F1: {:.4f}\".format(validation_macro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_macro:\n",
    "logistic_X_test_prediciton_macro=init_grid_search_macro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as macro parameter:\n",
    "precision_macro=precision_score(y_test,logistic_X_test_prediciton_macro,average='macro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_macro = recall_score(y_test,logistic_X_test_prediciton_macro,average='macro')\n",
    "print(\"Macro_Score Precision: {:.4f}\".format(precision_macro))\n",
    "print(\"Macro_Score Recall: {:.4f}\".format(recall_macro))\n",
    "for i in range(4):\n",
    "    f1_macro_i = f1_score(y_test[:,i],logistic_X_test_prediciton_macro[:,i],average='macro')\n",
    "    print(f\"{header[i+2]} Macro_Score F1: {f1_macro_i:.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#initialize GridSearchCV with scoring f1_micro:\n",
    "\n",
    "init_grid_search_micro=GridSearchCV(log_reg,params,cv=5,scoring='f1_micro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_micro:\n",
    "init_grid_search_micro.fit(X_train,y_train)\n",
    "\n",
    "#Validation Score with scoring f1_micro:\n",
    "validation_micro_score = init_grid_search_micro.score(X_test, y_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Micro f1:\n",
    "print(\"Micro_Score Validation F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_micro:\n",
    "logistic_X_test_prediciton_micro=init_grid_search_micro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as micro parameter:\n",
    "precision_micro=precision_score(y_test,logistic_X_test_prediciton_micro,average='micro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_micro = recall_score(y_test,logistic_X_test_prediciton_micro,average='micro')\n",
    "print(\"Micro_Score Precision: {:.4f}\".format(precision_micro))\n",
    "print(\"Micro_Score Recall: {:.4f}\".format(recall_micro))\n",
    "for i in range(4):\n",
    "    f1_micro_i = f1_score(y_test[:,i],logistic_X_test_prediciton_micro[:,i],average='micro')\n",
    "    print(f\"{header[i+2]} Micro_Score F1: {f1_micro_i:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6c1037-6428-47c6-8864-e1a640137517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro_Score Validation F1: 0.3034\n",
      "Macro_Score Precision: 0.3334\n",
      "Macro_Score Recall: 0.2923\n",
      "Gold Standards- Technology Macro_Score F1: 0.7765\n",
      "Gold Standards- Ride Share Macro_Score F1: 0.5211\n",
      "Gold Standards- Food Delivery Macro_Score F1: 0.5346\n",
      "Gold Standards- Online Shopping Macro_Score F1: 0.5179\n",
      "Micro_Score Validation F1: 0.6494\n",
      "Micro_Score Precision: 0.7692\n",
      "Micro_Score Recall: 0.5618\n",
      "Gold Standards- Technology Micro_Score F1: 0.7900\n",
      "Gold Standards- Ride Share Micro_Score F1: 0.8750\n",
      "Gold Standards- Food Delivery Micro_Score F1: 0.9350\n",
      "Gold Standards- Online Shopping Micro_Score F1: 0.8600\n"
     ]
    }
   ],
   "source": [
    "#modeling with ngram_range=(2,2) & LogisticRegression with CountVectorizer: \n",
    "#reading the necessary libraries:\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#np.random.seed(42)\n",
    "#import random\n",
    "#random.seed(42)\n",
    "\n",
    "# Generating Random multiplable classification for Logistic Regression:\n",
    "#X, y = make_multilabel_classification(n_classes=4,random_state=0)\n",
    "#lg_clf=LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "#clf = MultiOutputClassifier(lg_clf).fit(X, y)\n",
    "\n",
    "vec=TfidfVectorizer(ngram_range=(2,2))\n",
    "X_train_matrix =vec.fit_transform(X_txt) # This should be a matrix\n",
    "#y_matrix=vec.transform(y)# This should be a matrix\n",
    "\n",
    "#Split the data into training and test sets:\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train_matrix,y,test_size=0.2)\n",
    "\n",
    "#converting list to array:\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "#initializing logisticregression:\n",
    "log_reg=MultiOutputClassifier(LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000))\n",
    "\n",
    "#params with c values:\n",
    "params= {\"estimator__C\": [0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "#initialize GridSearchCV with scoring f1_macro:\n",
    "\n",
    "init_grid_search_macro=GridSearchCV(log_reg,params,cv=5,scoring='f1_macro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_macro:\n",
    "init_grid_search_macro.fit(X_train,y_train)\n",
    "\n",
    "#Validation Score with scoring f1_macro:\n",
    "validation_macro_score = init_grid_search_macro.score(X_test, y_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Macro f1:\n",
    "print(\"Macro_Score Validation F1: {:.4f}\".format(validation_macro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_macro:\n",
    "logistic_X_test_prediciton_macro=init_grid_search_macro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as macro parameter:\n",
    "precision_macro=precision_score(y_test,logistic_X_test_prediciton_macro,average='macro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_macro = recall_score(y_test,logistic_X_test_prediciton_macro,average='macro')\n",
    "print(\"Macro_Score Precision: {:.4f}\".format(precision_macro))\n",
    "print(\"Macro_Score Recall: {:.4f}\".format(recall_macro))\n",
    "for i in range(4):\n",
    "    f1_macro_i = f1_score(y_test[:,i],logistic_X_test_prediciton_macro[:,i],average='macro')\n",
    "    print(f\"{header[i+2]} Macro_Score F1: {f1_macro_i:.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#initialize GridSearchCV with scoring f1_micro:\n",
    "\n",
    "init_grid_search_micro=GridSearchCV(log_reg,params,cv=5,scoring='f1_micro')\n",
    "\n",
    "# Fit the model on X_train with scoring f1_micro:\n",
    "init_grid_search_micro.fit(X_train,y_train)\n",
    "\n",
    "#Validation Score with scoring f1_micro:\n",
    "validation_micro_score = init_grid_search_micro.score(X_test, y_test) \n",
    "\n",
    "# Get the score from the GridSearchCV \"best score\" with Micro f1:\n",
    "print(\"Micro_Score Validation F1: {:.4f}\".format(validation_micro_score))\n",
    "\n",
    "#predciting on X_test data with scoring f1_micro:\n",
    "logistic_X_test_prediciton_micro=init_grid_search_micro.predict(X_test)\n",
    "\n",
    "# Calculating precision, recall and f1 Scores with average as micro parameter:\n",
    "precision_micro=precision_score(y_test,logistic_X_test_prediciton_micro,average='micro')  # Get scores using logistic_X_test_prediciton and y_test with the precision_score method\n",
    "recall_micro = recall_score(y_test,logistic_X_test_prediciton_micro,average='micro')\n",
    "print(\"Micro_Score Precision: {:.4f}\".format(precision_micro))\n",
    "print(\"Micro_Score Recall: {:.4f}\".format(recall_micro))\n",
    "for i in range(4):\n",
    "    f1_micro_i = f1_score(y_test[:,i],logistic_X_test_prediciton_micro[:,i],average='micro')\n",
    "    print(f\"{header[i+2]} Micro_Score F1: {f1_micro_i:.4f}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
